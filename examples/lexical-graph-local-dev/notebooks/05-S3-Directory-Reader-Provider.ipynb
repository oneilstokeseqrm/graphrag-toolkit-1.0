{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb1535a",
   "metadata": {},
   "source": [
    "# 05 - S3 Directory Reader Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f529c1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.neo4j_graph_store_factory import Neo4jGraphStoreFactory\n",
    "\n",
    "GraphStoreFactory.register(Neo4jGraphStoreFactory)\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3_reader",
   "metadata": {},
   "source": [
    "## S3 Directory Reader Provider\n",
    "\n",
    "The S3 Directory reader provider reads documents from AWS S3 buckets using LlamaIndex's S3Reader.\n",
    "\n",
    "**Prerequisites:**\n",
    "- AWS credentials configured (AWS CLI, environment variables, or IAM role)\n",
    "- S3 bucket with readable documents\n",
    "- `boto3` package installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aws_setup",
   "metadata": {},
   "source": [
    "### AWS Credentials Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aws_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "\n",
    "def check_aws_credentials():\n",
    "    \"\"\"Check if AWS credentials are configured.\"\"\"\n",
    "    try:\n",
    "        # Try to create an S3 client\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Try to list buckets (minimal operation)\n",
    "        response = s3_client.list_buckets()\n",
    "        \n",
    "        print(\"AWS credentials are configured\")\n",
    "        print(f\"Found {len(response['Buckets'])} accessible S3 buckets\")\n",
    "        \n",
    "        # Show first few bucket names\n",
    "        if response['Buckets']:\n",
    "            print(\"\\nAccessible buckets:\")\n",
    "            for bucket in response['Buckets'][:5]:  # Show first 5\n",
    "                print(f\"  - {bucket['Name']}\")\n",
    "            if len(response['Buckets']) > 5:\n",
    "                print(f\"  ... and {len(response['Buckets']) - 5} more\")\n",
    "        \n",
    "        return True, response['Buckets']\n",
    "        \n",
    "    except NoCredentialsError:\n",
    "        print(\"AWS credentials not found\")\n",
    "        print(\"Configure credentials using one of:\")\n",
    "        print(\"  - AWS CLI: aws configure\")\n",
    "        print(\"  - Environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\")\n",
    "        print(\"  - IAM role (if running on EC2)\")\n",
    "        return False, []\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"AWS credentials error: {e}\")\n",
    "        return False, []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return False, []\n",
    "\n",
    "# Check AWS setup\n",
    "aws_configured, available_buckets = check_aws_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_test_bucket",
   "metadata": {},
   "source": [
    "### Create Test S3 Bucket and Files\n",
    "\n",
    "**Note:** This will create actual AWS resources. Make sure you have appropriate permissions and are aware of potential costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_test_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Only proceed if AWS is configured\n",
    "if aws_configured:\n",
    "    # Generate unique bucket name\n",
    "    bucket_name = f\"graphrag-test-{uuid.uuid4().hex[:8]}\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # Create bucket\n",
    "        print(f\"Creating test bucket: {bucket_name}\")\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "        \n",
    "        # Create test documents\n",
    "        test_documents = {\n",
    "            'documents/ai-overview.txt': '''\n",
    "Artificial Intelligence Overview\n",
    "\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create \n",
    "intelligent machines capable of performing tasks that typically require human intelligence.\n",
    "\n",
    "Key areas include:\n",
    "- Machine Learning\n",
    "- Natural Language Processing\n",
    "- Computer Vision\n",
    "- Robotics\n",
    "''',\n",
    "            'documents/ml-basics.txt': '''\n",
    "Machine Learning Basics\n",
    "\n",
    "Machine Learning is a subset of AI that enables computers to learn and improve \n",
    "from experience without being explicitly programmed.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Reinforcement Learning\n",
    "''',\n",
    "            'notes/project-notes.md': '''\n",
    "# Project Notes\n",
    "\n",
    "## Meeting Summary\n",
    "- Discussed AI implementation strategy\n",
    "- Reviewed technical requirements\n",
    "- Planned development timeline\n",
    "\n",
    "## Next Steps\n",
    "- Finalize architecture design\n",
    "- Begin prototype development\n",
    "- Schedule follow-up meetings\n",
    "''',\n",
    "            'data/sample-data.json': '''\n",
    "{\n",
    "  \"project\": \"GraphRAG Toolkit\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"components\": [\n",
    "    \"lexical-graph\",\n",
    "    \"readers\",\n",
    "    \"storage\"\n",
    "  ],\n",
    "  \"status\": \"active\"\n",
    "}\n",
    "'''\n",
    "        }\n",
    "        \n",
    "        # Upload test documents\n",
    "        print(\"Uploading test documents...\")\n",
    "        for key, content in test_documents.items():\n",
    "            s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=key,\n",
    "                Body=content.strip(),\n",
    "                ContentType='text/plain'\n",
    "            )\n",
    "            print(f\"  Uploaded: s3://{bucket_name}/{key}\")\n",
    "        \n",
    "        print(f\"\\nTest S3 bucket created: {bucket_name}\")\n",
    "        print(f\"Total files: {len(test_documents)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating test bucket: {e}\")\n",
    "        bucket_name = None\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping S3 bucket creation - AWS not configured\")\n",
    "    bucket_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_s3_reading",
   "metadata": {},
   "source": [
    "### S3 Reading with Key Prefix\n",
    "\n",
    "Read only files from a specific \"directory\" (key prefix) in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_s3_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import S3DirectoryReaderProvider, S3DirectoryReaderConfig\n",
    "\n",
    "if bucket_name:\n",
    "    # Configure S3 Reader to load a single file from S3\n",
    "    s3_config = S3DirectoryReaderConfig(\n",
    "        bucket=bucket_name,\n",
    "        key=\"documents/ai-overview.txt\",\n",
    "        metadata_fn=lambda s3_path: {\n",
    "            'source': 's3_file',\n",
    "            's3_path': s3_path,\n",
    "            'storage_type': 'cloud',\n",
    "            'reader_type': 'basic'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    s3_reader = S3DirectoryReaderProvider(s3_config)\n",
    "\n",
    "    try:\n",
    "        # Read one document from S3\n",
    "        s3_docs = s3_reader.read(None)\n",
    "\n",
    "        print(f\"Loaded {len(s3_docs)} document(s) from S3:\")\n",
    "        print(\"\\nS3 document details:\")\n",
    "        for i, doc in enumerate(s3_docs):\n",
    "            file_name = doc.metadata.get('file_name', 'unknown')\n",
    "            file_path = doc.metadata.get('file_path', 'unknown')\n",
    "            s3_path = doc.metadata.get('s3_path', 'unknown')\n",
    "            print(f\"  Document {i+1}: {file_name}\")\n",
    "            print(f\"    S3 Path: {s3_path}\")\n",
    "            print(f\"    Content preview: {doc.text[:80]}...\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from S3: {e}\")\n",
    "        s3_docs = []\n",
    "\n",
    "else:\n",
    "    print(\"Skipping S3 reading - no test bucket available\")\n",
    "    s3_docs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple_prefixes",
   "metadata": {},
   "source": [
    "### Reading Multiple S3 Prefixes\n",
    "\n",
    "Read from different \"directories\" in S3 with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple_prefixes_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import S3DirectoryReaderProvider, S3DirectoryReaderConfig\n",
    "\n",
    "if bucket_name:\n",
    "    # Define different S3 \"prefixes\" (directories) to read\n",
    "    s3_configs = {\n",
    "        'notes': S3DirectoryReaderConfig(\n",
    "            bucket=bucket_name,\n",
    "            prefix='notes/',\n",
    "            metadata_fn=lambda s3_path: {\n",
    "                'source': 's3_notes',\n",
    "                's3_path': s3_path,\n",
    "                'folder': 'notes',\n",
    "                'content_type': 'meeting_notes'\n",
    "            }\n",
    "        ),\n",
    "        'data': S3DirectoryReaderConfig(\n",
    "            bucket=bucket_name,\n",
    "            prefix='data/',\n",
    "            metadata_fn=lambda s3_path: {\n",
    "                'source': 's3_data',\n",
    "                's3_path': s3_path,\n",
    "                'folder': 'data',\n",
    "                'content_type': 'structured_data'\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    all_prefix_docs = []\n",
    "\n",
    "    for prefix_name, config in s3_configs.items():\n",
    "        try:\n",
    "            reader = S3DirectoryReaderProvider(config)\n",
    "            docs = reader.read(None)\n",
    "            all_prefix_docs.extend(docs)\n",
    "            \n",
    "            print(f\"Loaded {len(docs)} documents from '{prefix_name}/' prefix\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading '{prefix_name}/' prefix: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents from all prefixes: {len(all_prefix_docs)}\")\n",
    "    \n",
    "    # Group by content_type\n",
    "    content_types = {}\n",
    "    for doc in all_prefix_docs:\n",
    "        content_type = doc.metadata.get('content_type', 'unknown')\n",
    "        content_types[content_type] = content_types.get(content_type, 0) + 1\n",
    "\n",
    "    print(\"\\Content types summary:\")\n",
    "    for content_type, count in content_types.items():\n",
    "        print(f\"  - {content_type}: {count} documents\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping S3 prefix reading - no bucket name provided\")\n",
    "    all_prefix_docs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_s3_metadata",
   "metadata": {},
   "source": [
    "### Advanced S3 Metadata\n",
    "\n",
    "Create sophisticated metadata functions for S3 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc8dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from urllib.parse import urlparse\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import S3DirectoryReaderConfig, S3DirectoryReaderProvider\n",
    "\n",
    "def advanced_s3_metadata(s3_path: str) -> dict:\n",
    "    \"\"\"Extract detailed metadata from an S3 object path.\"\"\"\n",
    "    if not s3_path:\n",
    "        return {}\n",
    "\n",
    "    # Parse S3 URI or raw path\n",
    "    if s3_path.startswith(\"s3://\"):\n",
    "        parsed = urlparse(s3_path)\n",
    "        bucket = parsed.netloc\n",
    "        key = parsed.path.lstrip('/')\n",
    "    else:\n",
    "        parts = s3_path.split('/', 1)\n",
    "        bucket = parts[0] if len(parts) > 0 else 'unknown'\n",
    "        key = parts[1] if len(parts) > 1 else ''\n",
    "\n",
    "    # Split key into components\n",
    "    key_parts = key.split('/') if key else []\n",
    "    file_name = key_parts[-1] if key_parts else 'unknown'\n",
    "    folder_path = '/'.join(key_parts[:-1]) if len(key_parts) > 1 else ''\n",
    "    file_ext = '.' + file_name.split('.')[-1] if '.' in file_name else ''\n",
    "\n",
    "    # Base metadata\n",
    "    metadata = {\n",
    "        'source': 's3_advanced',\n",
    "        's3_bucket': bucket,\n",
    "        's3_key': key,\n",
    "        's3_full_path': f's3://{bucket}/{key}',\n",
    "        'file_name': file_name,\n",
    "        'file_extension': file_ext,\n",
    "        'folder_path': folder_path,\n",
    "        'processing_timestamp': datetime.datetime.now().isoformat(),\n",
    "        'storage_provider': 'aws_s3'\n",
    "    }\n",
    "\n",
    "    # Tag with content category\n",
    "    folder_lower = folder_path.lower()\n",
    "    if 'documents' in folder_lower:\n",
    "        metadata['content_category'] = 'documentation'\n",
    "    elif 'notes' in folder_lower:\n",
    "        metadata['content_category'] = 'meeting_notes'\n",
    "    elif 'data' in folder_lower:\n",
    "        metadata['content_category'] = 'structured_data'\n",
    "    else:\n",
    "        metadata['content_category'] = 'general'\n",
    "\n",
    "    return metadata\n",
    "\n",
    "# === Execute if bucket name is defined ===\n",
    "if bucket_name:\n",
    "    # Configure S3 reader using advanced metadata function\n",
    "    s3_config = S3DirectoryReaderConfig(\n",
    "        bucket=bucket_name,\n",
    "        prefix=\"documents/\",  # Change as needed\n",
    "        metadata_fn=advanced_s3_metadata\n",
    "    )\n",
    "\n",
    "    s3_reader = S3DirectoryReaderProvider(s3_config)\n",
    "\n",
    "    try:\n",
    "        docs = s3_reader.read(None)\n",
    "        print(f\"Loaded {len(docs)} documents from S3 with advanced metadata\")\n",
    "\n",
    "        if docs:\n",
    "            doc = docs[0]\n",
    "            print(\"Sample document metadata:\")\n",
    "            print(f\"  File name     : {doc.metadata.get('file_name')}\")\n",
    "            print(f\"  S3 path       : {doc.metadata.get('s3_full_path')}\")\n",
    "            print(f\"  Category      : {doc.metadata.get('content_category')}\")\n",
    "            print(f\"  All metadata  : {list(doc.metadata.keys())}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading documents with advanced metadata: {e}\")\n",
    "        docs = []\n",
    "\n",
    "else:\n",
    "    print(\"Skipping advanced metadata load â€” no bucket name provided\")\n",
    "    docs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indexing_s3_docs",
   "metadata": {},
   "source": [
    "### Index S3 Documents\n",
    "\n",
    "Index all the documents we've read from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indexing_s3_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bucket_name and any([s3_docs, all_prefix_docs]):\n",
    "    # Combine all S3 document lists safely\n",
    "    all_s3_docs = []\n",
    "    for doc_list in [s3_docs, all_prefix_docs]:\n",
    "        if doc_list:\n",
    "            all_s3_docs.extend(doc_list)\n",
    "\n",
    "    print(f\"Total S3 documents to index: {len(all_s3_docs)}\")\n",
    "\n",
    "    # Show document sources\n",
    "    sources = {}\n",
    "    for doc in all_s3_docs:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "    print(\"Document sources:\")\n",
    "    for source, count in sources.items():\n",
    "        print(f\"  {source}: {count} documents\")\n",
    "\n",
    "    # Index the documents\n",
    "    print(\"Indexing S3 documents...\")\n",
    "    try:\n",
    "        graph_index.extract_and_build(all_s3_docs, show_progress=True)\n",
    "        print(\"S3 documents indexed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error indexing S3 documents: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No S3 documents to index.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_s3",
   "metadata": {},
   "source": [
    "### Cleanup S3 Resources\n",
    "\n",
    "**Important:** Clean up the test S3 bucket to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_s3_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bucket_name:\n",
    "    try:\n",
    "        print(f\"Cleaning up S3 bucket: {bucket_name}\")\n",
    "        \n",
    "        # Delete all objects in bucket\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            objects_to_delete = [{'Key': obj['Key']} for obj in response['Contents']]\n",
    "            s3_client.delete_objects(\n",
    "                Bucket=bucket_name,\n",
    "                Delete={'Objects': objects_to_delete}\n",
    "            )\n",
    "            print(f\"  Deleted {len(objects_to_delete)} objects\")\n",
    "        \n",
    "        # Delete the bucket\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"Successfully deleted S3 bucket: {bucket_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up S3 bucket: {e}\")\n",
    "        print(f\"Please manually delete bucket: {bucket_name}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No S3 bucket to clean up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configuration_options",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "The S3DirectoryReaderConfig supports several options:\n",
    "\n",
    "### Basic Options\n",
    "- `bucket`: S3 bucket name (required)\n",
    "- `key`: S3 key prefix (optional, None reads entire bucket)\n",
    "- `aws_region`: AWS region (optional, uses default if not specified)\n",
    "\n",
    "### AWS Authentication\n",
    "- `aws_profile`: AWS profile name (optional)\n",
    "- Uses standard AWS credential chain if not specified\n",
    "\n",
    "### Metadata Enhancement\n",
    "- `metadata_fn`: Function to add custom metadata to each document\n",
    "\n",
    "### Example Configurations\n",
    "\n",
    "```python\n",
    "# Read entire bucket\n",
    "full_bucket_config = S3DirectoryReaderConfig(\n",
    "    bucket=\"my-documents-bucket\",\n",
    "    aws_region=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Read specific folder\n",
    "folder_config = S3DirectoryReaderConfig(\n",
    "    bucket=\"my-documents-bucket\",\n",
    "    key=\"documents/reports/\",\n",
    "    aws_region=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Use specific AWS profile\n",
    "profile_config = S3DirectoryReaderConfig(\n",
    "    bucket=\"my-documents-bucket\",\n",
    "    aws_profile=\"production\",\n",
    "    aws_region=\"us-west-2\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **AWS Credentials Not Found**\n",
    "   - Configure AWS CLI: `aws configure`\n",
    "   - Set environment variables: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`\n",
    "   - Use IAM roles if running on EC2\n",
    "\n",
    "2. **Access Denied Errors**\n",
    "   - Ensure your AWS credentials have S3 read permissions\n",
    "   - Check bucket policies and ACLs\n",
    "   - Verify the bucket exists and is in the correct region\n",
    "\n",
    "3. **No Documents Found**\n",
    "   - Verify the bucket name and key prefix\n",
    "   - Check if files exist in the specified S3 location\n",
    "   - Ensure files are readable (not encrypted with inaccessible keys)\n",
    "\n",
    "4. **Region Mismatch**\n",
    "   - Specify the correct AWS region in the configuration\n",
    "   - Some operations require region-specific endpoints\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use IAM roles instead of access keys when possible\n",
    "- Implement least-privilege access policies\n",
    "- Monitor S3 costs, especially for large buckets\n",
    "- Use key prefixes to organize and filter documents\n",
    "- Test with small buckets before processing large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completion",
   "metadata": {},
   "source": [
    "## Complete\n",
    "\n",
    "This notebook demonstrated the S3 Directory Reader Provider capabilities:\n",
    "\n",
    "### Key Features:\n",
    "1. **AWS Credentials Check**: Verify AWS configuration\n",
    "2. **Test Bucket Creation**: Create S3 resources for testing\n",
    "3. **Basic S3 Reading**: Read all files from a bucket\n",
    "4. **Prefix-based Reading**: Read from specific S3 \"directories\"\n",
    "5. **Multiple Prefix Reading**: Process different folders with different configs\n",
    "6. **Advanced Metadata**: Extract rich metadata from S3 paths\n",
    "7. **Resource Cleanup**: Properly clean up test resources\n",
    "\n",
    "### Use Cases:\n",
    "- **Document Archives**: Process existing document collections in S3\n",
    "- **Data Lakes**: Extract insights from structured data repositories\n",
    "- **Content Migration**: Import cloud-stored content into knowledge graphs\n",
    "- **Multi-tenant Systems**: Process documents from different S3 prefixes\n",
    "\n",
    "### Security Considerations:\n",
    "- Always use least-privilege IAM policies\n",
    "- Monitor S3 access logs\n",
    "- Be aware of data transfer costs\n",
    "- Clean up test resources to avoid charges\n",
    "\n",
    "The S3 Directory Reader Provider enables seamless integration with cloud-stored documents, making it easy to build knowledge graphs from existing S3 data repositories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
