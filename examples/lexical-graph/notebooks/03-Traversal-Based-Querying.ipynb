{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40dfbd7",
   "metadata": {},
   "source": [
    "# Traversal-Based Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9b60c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d08c74",
   "metadata": {},
   "source": [
    "### TraversalBasedRetriever\n",
    "\n",
    "See [TraversalBasedRetriever](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#traversalbasedretriever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import set_logging_config\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(f\"\"\"{response.response}\n",
    "\n",
    "retrieve_ms: {int(response.metadata['retrieve_ms'])}\n",
    "answer_ms  : {int(response.metadata['answer_ms'])}\n",
    "total_ms   : {int(response.metadata['total_ms'])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7ef25",
   "metadata": {},
   "source": [
    "#### Show the context passed to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62799e5b",
   "metadata": {},
   "source": [
    "#### Show the underlying results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for n in response.source_nodes:\n",
    "    print(json.dumps(n.metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5211757",
   "metadata": {},
   "source": [
    "#### Visualise the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.lexical_graph.retrieval.model import SearchResult\n",
    "\n",
    "def get_query_params_for_results(response, include_sources=True, include_facts=True, limit=-1):\n",
    "\n",
    "    statement_ids = []\n",
    "    source_params = []\n",
    "    fact_params = []\n",
    "    \n",
    "    nodes = response[:limit] if isinstance(response, list) else response.source_nodes[:limit]\n",
    "    \n",
    "    for n in nodes:\n",
    "        \n",
    "        search_result = SearchResult.model_validate(n.metadata)\n",
    "        source_id = search_result.source.sourceId\n",
    "        \n",
    "        for topic in search_result.topics:\n",
    "            \n",
    "            for statement in topic.statements:\n",
    "                \n",
    "                statement_id = statement.statementId\n",
    "                chunk_id = statement.chunkId\n",
    "                \n",
    "                statement_ids.append(statement_id)\n",
    "                if include_sources:\n",
    "                    source_params.append({'s': source_id, 'c': chunk_id, 'l': statement_id})\n",
    "                if include_facts:\n",
    "                    fact_params.append(statement_id)\n",
    "                    \n",
    "    \n",
    "    query_parameters = { \n",
    "        'statement_ids': statement_ids,\n",
    "        'source_params': source_params,\n",
    "        'fact_params': fact_params\n",
    "    }\n",
    "    \n",
    "    return query_parameters\n",
    "    \n",
    "query_parameters = get_query_params_for_results(response, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0007ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_var = '{\"__Source__\":\"url\",\"__Chunk__\":\"value\",\"__Topic__\":\"value\",\"__Statement__\":\"value\",\"__Fact__\":\"value\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f42b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc --query-parameters query_parameters -d $display_var -l 20\n",
    "\n",
    "UNWIND $source_params AS source_params\n",
    "MATCH p=(s:`__Source__`)<--(c:`__Chunk__`)<--(t:`__Topic__`)<--(l:`__Statement__`)\n",
    "WHERE id(s) = source_params.s \n",
    "    AND id(c) = source_params.c \n",
    "    AND id(l) = source_params.l\n",
    "RETURN p\n",
    "UNION\n",
    "MATCH p=(x:`__Source__`)<--(:`__Chunk__`)<--(:`__Topic__`)<--(l:`__Statement__`)<-[:`__SUPPORTS__`]-(:`__Fact__`)-[:`__NEXT__`*0..1]->(:`__Fact__`)-[:`__SUPPORTS__`]->(ll:`__Statement__`)-->(:`__Topic__`)-->(:`__Chunk__`)-->(y:`__Source__`)\n",
    "WHERE id(l) IN $fact_params\n",
    "    AND id(ll) IN $fact_params\n",
    "    AND x <> y\n",
    "RETURN p\n",
    "UNION\n",
    "MATCH p=(l:`__Statement__`)\n",
    "WHERE id(l) IN $statement_ids\n",
    "RETURN p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378f518",
   "metadata": {},
   "source": [
    "#### Metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import set_logging_config\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.metadata import FilterConfig\n",
    "\n",
    "from llama_index.core.vector_stores.types import FilterOperator, MetadataFilter\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    filter_config = FilterConfig(\n",
    "        MetadataFilter(\n",
    "            key='url',\n",
    "            value='https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "            operator=FilterOperator.EQ\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(f\"\"\"{response.response}\n",
    "\n",
    "retrieve_ms: {int(response.metadata['retrieve_ms'])}\n",
    "answer_ms  : {int(response.metadata['answer_ms'])}\n",
    "total_ms   : {int(response.metadata['total_ms'])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41035822",
   "metadata": {},
   "source": [
    "#### Set subretriever\n",
    "\n",
    "In the example below, the `TraversalBasedRetriever` is configured with a `ChunkBasedSearch` subretriever. (You can also try with `EntityBasedSearch` and `EntityContextSearch`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d04b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import ChunkBasedSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import EntityBasedSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import EntityContextSearch\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[EntityContextSearch]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(f\"\"\"{response.response}\n",
    "\n",
    "retrieve_ms: {int(response.metadata['retrieve_ms'])}\n",
    "answer_ms  : {int(response.metadata['answer_ms'])}\n",
    "total_ms   : {int(response.metadata['total_ms'])}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
