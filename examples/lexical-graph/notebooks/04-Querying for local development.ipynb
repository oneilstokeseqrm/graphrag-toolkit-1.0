{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40dfbd7",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9b60c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d08c74",
   "metadata": {},
   "source": [
    "### TraversalBasedRetriever\n",
    "\n",
    "See [TraversalBasedRetriever](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#traversalbasedretriever)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check GPU support on Ubuntu 24.04 LTS",
   "id": "9bec686fd2672567"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:53:53.968088Z",
     "start_time": "2025-04-26T21:53:53.965170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Force inject paths (adapt to your system if different)\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"EXTRACTION_MODEL\"] = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "os.environ[\"RESPONSE_MODEL\"] = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "os.environ[\"EMBEDDINGS_MODEL\"] = \"cohere.embed-english-v3\"\n",
    "os.environ[\"EMBEDDINGS_DIMENSIONS\"] = \"1024\"\n",
    "os.environ[\"EXTRACTION_NUM_WORKERS\"] = \"2\"\n",
    "os.environ[\"EXTRACTION_BATCH_SIZE\"] = \"4\"\n",
    "os.environ[\"BUILD_NUM_WORKERS\"] = \"2\"\n",
    "os.environ[\"BUILD_BATCH_WRITE_SIZE\"] = \"25\"\n",
    "os.environ[\"BATCH_WRITES_ENABLED\"] = \"True\"\n",
    "\n",
    "# Re-check CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version (Torch):\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"GPU still not detected inside notebook\")"
   ],
   "id": "2b7107a66a3d6607",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "CUDA Version (Torch): 12.6\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup AWS Profile",
   "id": "cf73d0b003bf1179"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:53:54.014860Z",
     "start_time": "2025-04-26T21:53:54.013657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure AWS Profile and Region\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "\n",
    "# Assign profile and region to GraphRAGConfig\n",
    "GraphRAGConfig.aws_profile = \"padmin\" #Optional, use if using AWS SSO\n",
    "GraphRAGConfig.aws_region = \"us-east-1\""
   ],
   "id": "d0895acf6fbc98f1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup model",
   "id": "d817c4c9c68f116b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:53:54.058489Z",
     "start_time": "2025-04-26T21:53:54.057433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Claude model via Bedrock\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ],
   "id": "4246fd3faec4a7e5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup BedrockConverse",
   "id": "1f3e98634dc4c67d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:53:54.130702Z",
     "start_time": "2025-04-26T21:53:54.102318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure BedrockConverse\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "\n",
    "try:\n",
    "    GraphRAGConfig.extraction_llm = BedrockConverse.from_json(f'''\n",
    "    {{\n",
    "        \"model\": \"{model_id}\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"profile_name\": \"{GraphRAGConfig.aws_profile}\",\n",
    "        \"region_name\": \"{GraphRAGConfig.aws_region}\"\n",
    "    }}\n",
    "    ''')\n",
    "    print(f\"Successfully configured Bedrock model: {model_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize BedrockConverse: {str(e)}\")\n",
    "    raise\n",
    "### Display LLM Configuration"
   ],
   "id": "3e1dcab3be3bf81a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully configured Bedrock model: us.anthropic.claude-3-5-sonnet-20240620-v1:0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Display LLM Configuration",
   "id": "23ad3e6f57be3dee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:53:54.149794Z",
     "start_time": "2025-04-26T21:53:54.148207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display LLM configuration\n",
    "llm = GraphRAGConfig.extraction_llm\n",
    "print(\"LLM class:\", llm.__class__.__name__)\n",
    "print(\"Model ID:\", llm.model)\n",
    "print(\"Temperature:\", llm.temperature)\n",
    "print(\"Max tokens:\", llm.max_tokens)\n",
    "print(\"Profile:\", getattr(llm, 'profile_name', None))\n",
    "print(\"Region:\", getattr(llm, 'region_name', None))"
   ],
   "id": "cd6de0bcdbaf30c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM class: BedrockConverse\n",
      "Model ID: us.anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Temperature: 0.0\n",
      "Max tokens: 4096\n",
      "Profile: padmin\n",
      "Region: us-east-1\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:53:54.195067Z",
     "start_time": "2025-04-26T21:53:54.193804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"embed_model model name = {GraphRAGConfig.embed_model.model_name}\")\n",
    "print(f\"embed_model dimension  = {GraphRAGConfig.embed_dimensions}\")"
   ],
   "id": "d71998f8626eb2e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_model model name = amazon.titan-embed-text-v1\n",
      "embed_model dimension  = 1536\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup GraphRag Config",
   "id": "f686fd44adf0037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:58:15.790164Z",
     "start_time": "2025-04-26T21:58:15.787968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup GraphRag Config\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "import os\n",
    "from graphrag_toolkit.lexical_graph.config import GraphRAGConfig\n",
    "\n",
    "def setup_graphrag_config() -> None:\n",
    "    \"\"\"\n",
    "    Inject BedrockEmbedding into existing GraphRAGConfig without resetting other config values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        region = GraphRAGConfig.aws_region or os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "        profile = GraphRAGConfig.aws_profile or os.getenv(\"AWS_PROFILE\")\n",
    "\n",
    "        # Request to use v1\n",
    "        requested_model_name = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "        print(f\"[DEBUG] Requesting BedrockEmbedding with model: {requested_model_name}\")\n",
    "\n",
    "        embed_model = BedrockEmbedding(\n",
    "            model=requested_model_name,\n",
    "            region=region,\n",
    "            profile_name=profile\n",
    "        )\n",
    "\n",
    "        # Assign to GraphRAGConfig\n",
    "        GraphRAGConfig.embed_model = embed_model\n",
    "        GraphRAGConfig.embed_dimensions = 1536\n",
    "\n",
    "        # Print actual model info after Bedrock client initialized\n",
    "        actual_model_name = GraphRAGConfig.embed_model.model_name\n",
    "        print(f\"[CONFIRM] embed_model model name = {actual_model_name}\")\n",
    "        print(f\"[CONFIRM] embed_model dimension  = {GraphRAGConfig.embed_dimensions}\")\n",
    "\n",
    "        if actual_model_name != requested_model_name:\n",
    "            print(f\"[WARNING] Bedrock returned model '{actual_model_name}', expected '{requested_model_name}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to update GraphRAGConfig: {str(e)}\")\n",
    "        raise\n"
   ],
   "id": "bdc53c0e9b307d6d",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with PostgreSQL",
   "id": "b03ac415378e2820"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:54:46.853458Z",
     "start_time": "2025-04-26T21:54:46.851524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to PostgreSQL Vector Store\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "# PostgreSQL connection string\n",
    "postgre_connection_info = 'postgresql://graphrag:graphragpass@localhost:5432/graphrag_db'\n",
    "\n",
    "# Instantiate vector store using factory\n",
    "vector_store = VectorStoreFactory.for_vector_store(postgre_connection_info)\n",
    "\n",
    "# Optional: confirm\n",
    "print(f\"Vector store initialized: {vector_store}\")"
   ],
   "id": "862a9fe3c0c61e6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized: indexes={'chunk': PGIndex(index_name='chunk', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1536, embed_model=BedrockEmbedding(model_name='amazon.titan-embed-text-v1', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x78e054c5b890>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name=None, botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False), 'statement': PGIndex(index_name='statement', tenant_id=TenantId(value=None), writeable=True, database='graphrag_db', schema_name='graphrag', host='localhost', port=5432, username='graphrag', password='graphragpass', dimensions=1536, embed_model=BedrockEmbedding(model_name='amazon.titan-embed-text-v1', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x78e054c5b890>, num_workers=None, profile_name='padmin', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name=None, botocore_session=None, botocore_config=None, max_retries=10, timeout=60.0, additional_kwargs={}), enable_iam_db_auth=False, initialized=False)}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup connection with FalkorDB",
   "id": "5fdb3f1e78c21337"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:54:51.705839Z",
     "start_time": "2025-04-26T21:54:49.628248Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip#subdirectory=lexical-graph-contrib/falkordb",
   "id": "4c3c51fc31e11696",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip#subdirectory=lexical-graph-contrib/falkordb\r\n",
      "  Using cached https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.4.0.zip\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: FalkorDB in /home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages (from graphrag-toolkit-lexical-graph-falkordb==1.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: redis in /home/evanerwee/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages (from graphrag-toolkit-lexical-graph-falkordb==1.0.0) (5.2.1)\r\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:54:54.965865Z",
     "start_time": "2025-04-26T21:54:54.963715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to FalkorDB Graph Store\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "\n",
    "# Connection string for FalkorDB\n",
    "falkordb_connection_info = 'falkordb://localhost:6379'\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "# Instantiate a graph store using the factory\n",
    "graph_store = GraphStoreFactory.for_graph_store(falkordb_connection_info)\n",
    "\n",
    "# Optional: confirm initialization\n",
    "print(f\"FalkorDB GraphStore initialized: {graph_store}\")"
   ],
   "id": "a91526d4dd20c426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalkorDB GraphStore initialized: log_formatting=RedactedGraphQueryLogFormatting() tenant_id=TenantId(value=None) endpoint_url='localhost:6379' database='graphrag' username=None password=None ssl=False\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "2ec2a722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:58:25.223732Z",
     "start_time": "2025-04-26T21:58:23.077019Z"
    }
   },
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import set_logging_config\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ðŸ› ï¸ MUST: Setup the embeddings before anything else\n",
    "setup_graphrag_config()\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "\n",
    "# Now create query engine\n",
    "query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")\n",
    "\n",
    "# Now you can query\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "print(response.response)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n",
      "[DEBUG] Requesting BedrockEmbedding with model: amazon.titan-embed-text-v1\n",
      "[CONFIRM] embed_model model name = amazon.titan-embed-text-v1\n",
      "[CONFIRM] embed_model dimension  = 1536\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     21\u001B[39m query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n\u001B[32m     22\u001B[39m     graph_store,\n\u001B[32m     23\u001B[39m     vector_store\n\u001B[32m     24\u001B[39m )\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# Now you can query\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m response = query_engine.query(\u001B[33m\"\u001B[39m\u001B[33mWhat are the differences between Neptune Database and Neptune Analytics?\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     28\u001B[39m \u001B[38;5;28mprint\u001B[39m(response.response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     result = func(*args, **kwargs)\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    324\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    325\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py:52\u001B[39m, in \u001B[36mBaseQueryEngine.query\u001B[39m\u001B[34m(self, str_or_query_bundle)\u001B[39m\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     51\u001B[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m     query_result = \u001B[38;5;28mself\u001B[39m._query(str_or_query_bundle)\n\u001B[32m     53\u001B[39m dispatcher.event(\n\u001B[32m     54\u001B[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001B[32m     55\u001B[39m )\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m query_result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     result = func(*args, **kwargs)\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    324\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    325\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/lexical_graph_query_engine.py:249\u001B[39m, in \u001B[36mLexicalGraphQueryEngine._query\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m    245\u001B[39m start = time.time()\n\u001B[32m    247\u001B[39m query_bundle = to_embedded_query(query_bundle, GraphRAGConfig.embed_model)\n\u001B[32m--> \u001B[39m\u001B[32m249\u001B[39m results = \u001B[38;5;28mself\u001B[39m.retriever.retrieve(query_bundle)\n\u001B[32m    251\u001B[39m end_retrieve = time.time()\n\u001B[32m    253\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m post_processor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.post_processors:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     result = func(*args, **kwargs)\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    324\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    325\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/base/base_retriever.py:245\u001B[39m, in \u001B[36mBaseRetriever.retrieve\u001B[39m\u001B[34m(self, str_or_query_bundle)\u001B[39m\n\u001B[32m    240\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.callback_manager.as_trace(\u001B[33m\"\u001B[39m\u001B[33mquery\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.callback_manager.event(\n\u001B[32m    242\u001B[39m         CBEventType.RETRIEVE,\n\u001B[32m    243\u001B[39m         payload={EventPayload.QUERY_STR: query_bundle.query_str},\n\u001B[32m    244\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m retrieve_event:\n\u001B[32m--> \u001B[39m\u001B[32m245\u001B[39m         nodes = \u001B[38;5;28mself\u001B[39m._retrieve(query_bundle)\n\u001B[32m    246\u001B[39m         nodes = \u001B[38;5;28mself\u001B[39m._handle_recursive_retrieval(query_bundle, nodes)\n\u001B[32m    247\u001B[39m         retrieve_event.on_end(\n\u001B[32m    248\u001B[39m             payload={EventPayload.NODES: nodes},\n\u001B[32m    249\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     result = func(*args, **kwargs)\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    324\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    325\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/retrieval/retrievers/traversal_based_base_retriever.py:92\u001B[39m, in \u001B[36mTraversalBasedBaseRetriever._retrieve\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m     89\u001B[39m start_retrieve = time.time()\n\u001B[32m     91\u001B[39m start_node_ids = \u001B[38;5;28mself\u001B[39m.get_start_node_ids(query_bundle)\n\u001B[32m---> \u001B[39m\u001B[32m92\u001B[39m search_results:SearchResultCollection = \u001B[38;5;28mself\u001B[39m.do_graph_search(query_bundle, start_node_ids)\n\u001B[32m     94\u001B[39m end_retrieve = time.time()\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m processor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.processors:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     result = func(*args, **kwargs)\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    324\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    325\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/retrieval/retrievers/composite_traversal_based_retriever.py:143\u001B[39m, in \u001B[36mCompositeTraversalBasedRetriever.do_graph_search\u001B[39m\u001B[34m(self, query_bundle, start_node_ids)\u001B[39m\n\u001B[32m    133\u001B[39m subqueries = (\u001B[38;5;28mself\u001B[39m.query_decomposition.decompose_query(query_bundle) \n\u001B[32m    134\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.derive_subqueries \n\u001B[32m    135\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m [query_bundle]\n\u001B[32m    136\u001B[39m )\n\u001B[32m    138\u001B[39m tasks = [\n\u001B[32m    139\u001B[39m     \u001B[38;5;28mself\u001B[39m._get_search_results_for_query(subquery) \n\u001B[32m    140\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m subquery \u001B[38;5;129;01min\u001B[39;00m subqueries\n\u001B[32m    141\u001B[39m ]\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m task_results:List[SearchResultCollection] = run_async_tasks(tasks)\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m task_result \u001B[38;5;129;01min\u001B[39;00m task_results:\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m search_result \u001B[38;5;129;01min\u001B[39;00m task_result.results:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/async_utils.py:77\u001B[39m, in \u001B[36mrun_async_tasks\u001B[39m\u001B[34m(tasks, show_progress, progress_bar_desc)\u001B[39m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_gather\u001B[39m() -> List[Any]:\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(*tasks_to_execute)\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m outputs: List[Any] = asyncio_run(_gather())\n\u001B[32m     78\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/async_utils.py:33\u001B[39m, in \u001B[36masyncio_run\u001B[39m\u001B[34m(coro)\u001B[39m\n\u001B[32m     30\u001B[39m     loop = asyncio.get_event_loop()\n\u001B[32m     32\u001B[39m     \u001B[38;5;66;03m# If we're here, there's an existing loop but it's not running\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loop.run_until_complete(coro)\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     36\u001B[39m     \u001B[38;5;66;03m# If we can't get the event loop, we're likely in a different thread, or its already running\u001B[39;00m\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/nest_asyncio.py:98\u001B[39m, in \u001B[36m_patch_loop.<locals>.run_until_complete\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f.done():\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m     97\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mEvent loop stopped before Future completed.\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m f.result()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/asyncio/futures.py:202\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    200\u001B[39m \u001B[38;5;28mself\u001B[39m.__log_traceback = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m202\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception.with_traceback(\u001B[38;5;28mself\u001B[39m._exception_tb)\n\u001B[32m    203\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/asyncio/tasks.py:316\u001B[39m, in \u001B[36mTask.__step_run_and_handle_result\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    314\u001B[39m         result = coro.send(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    315\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m         result = coro.throw(exc)\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._must_cancel:\n\u001B[32m    319\u001B[39m         \u001B[38;5;66;03m# Task is cancelled right before coro stops.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/async_utils.py:75\u001B[39m, in \u001B[36mrun_async_tasks.<locals>._gather\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_gather\u001B[39m() -> List[Any]:\n\u001B[32m---> \u001B[39m\u001B[32m75\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(*tasks_to_execute)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/asyncio/tasks.py:385\u001B[39m, in \u001B[36mTask.__wakeup\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m    383\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__wakeup\u001B[39m(\u001B[38;5;28mself\u001B[39m, future):\n\u001B[32m    384\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m385\u001B[39m         future.result()\n\u001B[32m    386\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    387\u001B[39m         \u001B[38;5;66;03m# This may also be a cancellation.\u001B[39;00m\n\u001B[32m    388\u001B[39m         \u001B[38;5;28mself\u001B[39m.__step(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/asyncio/tasks.py:314\u001B[39m, in \u001B[36mTask.__step_run_and_handle_result\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    311\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    312\u001B[39m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[32m    313\u001B[39m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m314\u001B[39m         result = coro.send(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    315\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    316\u001B[39m         result = coro.throw(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/retrieval/retrievers/composite_traversal_based_retriever.py:123\u001B[39m, in \u001B[36mCompositeTraversalBasedRetriever._get_search_results_for_query\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m    120\u001B[39m     executor.shutdown()\n\u001B[32m    122\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m future \u001B[38;5;129;01min\u001B[39;00m futures:\n\u001B[32m--> \u001B[39m\u001B[32m123\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m scored_node \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m future.result():\n\u001B[32m    124\u001B[39m             search_results.append(SearchResult.model_validate_json(scored_node.node.text))\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m SearchResultCollection(results=search_results, entities=entities)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001B[39m, in \u001B[36mDispatcher.span.<locals>.async_wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    360\u001B[39m \u001B[38;5;28mself\u001B[39m.span_enter(\n\u001B[32m    361\u001B[39m     id_=id_,\n\u001B[32m    362\u001B[39m     bound_args=bound_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m    365\u001B[39m     tags=tags,\n\u001B[32m    366\u001B[39m )\n\u001B[32m    367\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m368\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m func(*args, **kwargs)\n\u001B[32m    369\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    370\u001B[39m     \u001B[38;5;28mself\u001B[39m.event(SpanDropEvent(span_id=id_, err_str=\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/base/base_retriever.py:276\u001B[39m, in \u001B[36mBaseRetriever.aretrieve\u001B[39m\u001B[34m(self, str_or_query_bundle)\u001B[39m\n\u001B[32m    271\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.callback_manager.as_trace(\u001B[33m\"\u001B[39m\u001B[33mquery\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    272\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.callback_manager.event(\n\u001B[32m    273\u001B[39m         CBEventType.RETRIEVE,\n\u001B[32m    274\u001B[39m         payload={EventPayload.QUERY_STR: query_bundle.query_str},\n\u001B[32m    275\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m retrieve_event:\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m         nodes = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._aretrieve(query_bundle=query_bundle)\n\u001B[32m    277\u001B[39m         nodes = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._ahandle_recursive_retrieval(\n\u001B[32m    278\u001B[39m             query_bundle=query_bundle, nodes=nodes\n\u001B[32m    279\u001B[39m         )\n\u001B[32m    280\u001B[39m         retrieve_event.on_end(\n\u001B[32m    281\u001B[39m             payload={EventPayload.NODES: nodes},\n\u001B[32m    282\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/base/base_retriever.py:307\u001B[39m, in \u001B[36mBaseRetriever._aretrieve\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m    301\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_aretrieve\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) -> List[NodeWithScore]:\n\u001B[32m    302\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Asynchronously retrieve nodes given query.\u001B[39;00m\n\u001B[32m    303\u001B[39m \n\u001B[32m    304\u001B[39m \u001B[33;03m    Implemented by the user.\u001B[39;00m\n\u001B[32m    305\u001B[39m \n\u001B[32m    306\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve(query_bundle)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     result = func(*args, **kwargs)\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    324\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    325\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/retrieval/retrievers/traversal_based_base_retriever.py:91\u001B[39m, in \u001B[36mTraversalBasedBaseRetriever._retrieve\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m     87\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m] Begin retrieve [args: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.args.to_dict()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m]\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     89\u001B[39m start_retrieve = time.time()\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m start_node_ids = \u001B[38;5;28mself\u001B[39m.get_start_node_ids(query_bundle)\n\u001B[32m     92\u001B[39m search_results:SearchResultCollection = \u001B[38;5;28mself\u001B[39m.do_graph_search(query_bundle, start_node_ids)\n\u001B[32m     94\u001B[39m end_retrieve = time.time()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     result = func(*args, **kwargs)\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    324\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    325\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/retrieval/retrievers/chunk_based_search.py:56\u001B[39m, in \u001B[36mChunkBasedSearch.get_start_node_ids\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_start_node_ids\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) -> List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m     54\u001B[39m     logger.debug(\u001B[33m'\u001B[39m\u001B[33mGetting start node ids for chunk-based search...\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m     chunks = get_diverse_vss_elements(\u001B[33m'\u001B[39m\u001B[33mchunk\u001B[39m\u001B[33m'\u001B[39m, query_bundle, \u001B[38;5;28mself\u001B[39m.vector_store, \u001B[38;5;28mself\u001B[39m.args)\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [chunk[\u001B[33m'\u001B[39m\u001B[33mchunk\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mchunkId\u001B[39m\u001B[33m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunks]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/retrieval/utils/vector_utils.py:24\u001B[39m, in \u001B[36mget_diverse_vss_elements\u001B[39m\u001B[34m(index_name, query_bundle, vector_store, args)\u001B[39m\n\u001B[32m     20\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m vector_store.get_index(index_name).top_k(query_bundle, top_k=vss_top_k)\n\u001B[32m     22\u001B[39m top_k = vss_top_k * diversity_factor\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m elements = vector_store.get_index(index_name).top_k(query_bundle, top_k=top_k)\n\u001B[32m     26\u001B[39m source_map = {}\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m elements:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/storage/vector/pg_vector_indexes.py:248\u001B[39m, in \u001B[36mPGIndex.top_k\u001B[39m\u001B[34m(self, query_bundle, top_k)\u001B[39m\n\u001B[32m    239\u001B[39m     cur.execute(\u001B[33mf\u001B[39m\u001B[33m'''\u001B[39m\u001B[33mSELECT \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.index_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mId, metadata, embedding <-> %s AS score\u001B[39m\n\u001B[32m    240\u001B[39m \u001B[33m        FROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.schema_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.underlying_index_name()\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m    241\u001B[39m \u001B[33m        ORDER BY score ASC LIMIT %s;\u001B[39m\u001B[33m'''\u001B[39m,\n\u001B[32m    242\u001B[39m         (np.array(query_bundle.embedding), top_k)\n\u001B[32m    243\u001B[39m     )\n\u001B[32m    245\u001B[39m     results = cur.fetchall()\n\u001B[32m    247\u001B[39m     top_k_results.extend(\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m         [\u001B[38;5;28mself\u001B[39m._to_top_k_result(result) \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m results]\n\u001B[32m    249\u001B[39m     )\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m UndefinedTable \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    252\u001B[39m     logger.warning(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mIndex \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.underlying_index_name()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m does not exist\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/site-packages/graphrag_toolkit/lexical_graph/storage/vector/pg_vector_indexes.py:196\u001B[39m, in \u001B[36mPGIndex._to_top_k_result\u001B[39m\u001B[34m(self, r)\u001B[39m\n\u001B[32m    190\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_to_top_k_result\u001B[39m(\u001B[38;5;28mself\u001B[39m, r):\n\u001B[32m    192\u001B[39m     result = {\n\u001B[32m    193\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mscore\u001B[39m\u001B[33m'\u001B[39m: \u001B[38;5;28mround\u001B[39m(r[\u001B[32m2\u001B[39m], \u001B[32m7\u001B[39m)\n\u001B[32m    194\u001B[39m     }\n\u001B[32m--> \u001B[39m\u001B[32m196\u001B[39m     metadata = json.loads(r[\u001B[32m1\u001B[39m])\n\u001B[32m    198\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m INDEX_KEY \u001B[38;5;129;01min\u001B[39;00m metadata:\n\u001B[32m    199\u001B[39m         index_name = metadata[INDEX_KEY][\u001B[33m'\u001B[39m\u001B[33mindex\u001B[39m\u001B[33m'\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/graphrag-toolkit4/lib/python3.12/json/__init__.py:339\u001B[39m, in \u001B[36mloads\u001B[39m\u001B[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[39m\n\u001B[32m    337\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    338\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(s, (\u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mbytearray\u001B[39m)):\n\u001B[32m--> \u001B[39m\u001B[32m339\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mthe JSON object must be str, bytes or bytearray, \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    340\u001B[39m                         \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mnot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m    341\u001B[39m     s = s.decode(detect_encoding(s), \u001B[33m'\u001B[39m\u001B[33msurrogatepass\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    344\u001B[39m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    345\u001B[39m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n",
      "\u001B[31mTypeError\u001B[39m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "3b5a947e",
   "metadata": {},
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb78418c",
   "metadata": {},
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41035822",
   "metadata": {},
   "source": [
    "#### Set subretriever\n",
    "\n",
    "In the example below, the `TraversalBasedRetriever` is configured with a `ChunkBasedSearch` subretriever."
   ]
  },
  {
   "cell_type": "code",
   "id": "90d04b97",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import ChunkBasedSearch\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[ChunkBasedSearch]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d174f696",
   "metadata": {},
   "source": [
    "### SemanticGuidedRetriever\n",
    "\n",
    "See [SemanticGuidedRetriever](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#semanticguidedretriever)."
   ]
  },
  {
   "cell_type": "code",
   "id": "9d1f3184",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3416845",
   "metadata": {},
   "source": [
    "#### Set subretrievers"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec1adddb",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import StatementCosineSimilaritySearch, KeywordRankingSearch, SemanticBeamGraphSearch\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        StatementCosineSimilaritySearch, \n",
    "        KeywordRankingSearch, \n",
    "        SemanticBeamGraphSearch\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b234dc61",
   "metadata": {},
   "source": [
    "#### Reranking beam search (CPU)\n",
    "\n",
    "The example below uses a `SentenceReranker` with a `RerankingBeamGraphSearch` to rerank statements while conducting the beam search."
   ]
  },
  {
   "cell_type": "code",
   "id": "902acaef",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import RerankingBeamGraphSearch, StatementCosineSimilaritySearch, KeywordRankingSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = SentenceReranker(\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        cosine_retriever,\n",
    "        keyword_retriever,\n",
    "        beam_retriever\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "717049be",
   "metadata": {},
   "source": [
    "#### Reranking beam search (GPU)\n",
    "\n",
    "The example below uses a `BGEReranker` with a `RerankingBeamGraphSearch` to rerank statements while conducting the beam search.\n",
    "\n",
    "There will be a delay the first time this runs while the reranker downloads tensors."
   ]
  },
  {
   "cell_type": "code",
   "id": "06c3b363",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import RerankingBeamGraphSearch, StatementCosineSimilaritySearch, KeywordRankingSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors.bge_reranker import BGEReranker\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = BGEReranker(\n",
    "    gpu_id=0,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        cosine_retriever,\n",
    "        keyword_retriever,\n",
    "        beam_retriever\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ba2ac35",
   "metadata": {},
   "source": [
    "#### Post-processors \n",
    "\n",
    "The example below uses a `StatementDiversityPostProcessor`, `SentenceReranker` and `StatementEnhancementPostProcessor`.\n",
    "\n",
    "  - `SentenceReranker` - Reranks statements using the `mixedbread-ai/mxbai-rerank-xsmall-v1` model. \n",
    "\n",
    "  - `StatementEnhancementPostProcessor` - Enhances statements by using chunk context and an LLM to improve content while preserving original metadata.\n",
    "\n",
    "  - `StatementDiversityPostProcessor` - Removes similar statements using TF-IDF similarity with a default threshold of 0.975 to ensure diversity in the processed nodes.\n",
    "\n",
    "Before running `StatementDiversityPostProcessor` for the first time, load the following package:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "If you're running on a GPU device, you can replace the `SentenceReranker` with a `BGEReranker`, which reranks statements using the ``BAAI/bge-reranker-v2-minicpm-layerwise`` model."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f2d46a3",
   "metadata": {},
   "source": [
    "!python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef08819d",
   "metadata": {},
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker, StatementDiversityPostProcessor, StatementEnhancementPostProcessor\n",
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    post_processors=[\n",
    "        SentenceReranker(), \n",
    "        StatementDiversityPostProcessor(), \n",
    "        StatementEnhancementPostProcessor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
