# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import logging
import multiprocessing
import math
from typing import Any, List, Optional, Iterable
from pipe import Pipe

from graphrag_toolkit.lexical_graph import TenantId
from graphrag_toolkit.lexical_graph.config import GraphRAGConfig
from graphrag_toolkit.lexical_graph.metadata import SourceMetadataFormatter, DefaultSourceMetadataFormatter
from graphrag_toolkit.lexical_graph.indexing import NodeHandler, IdGenerator
from graphrag_toolkit.lexical_graph.indexing.utils.pipeline_utils import run_pipeline
from graphrag_toolkit.lexical_graph.indexing.model import SourceType, SourceDocument, source_documents_from_source_types
from graphrag_toolkit.lexical_graph.indexing.build.node_builder import NodeBuilder
from graphrag_toolkit.lexical_graph.indexing.build.checkpoint import Checkpoint, CheckpointWriter
from graphrag_toolkit.lexical_graph.indexing.build.node_builders import NodeBuilders
from graphrag_toolkit.lexical_graph.indexing.build.build_filters import BuildFilters

from llama_index.core.utils import iter_batch
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.schema import TransformComponent, BaseNode

logger = logging.getLogger(__name__)

class NodeFilter(TransformComponent):
      
    def __call__(self, nodes: List[BaseNode], **kwargs: Any) -> List[BaseNode]:
        return nodes

class BuildPipeline():
    """
    Represents a configurable build pipeline designed for efficiently processing and transforming
    data into nodes through various components, while supporting batching, parallelization,
    and customizations.

    The BuildPipeline class provides a mechanism to process collections of input data using a
    series of transformation components. It supports batch processing, parallel execution with
    a configurable number of workers, and optional input/output customization through builders
    and filters. It is tailored for scalable processing needs, such as in data ingestion frameworks,
    where transformations and filtering of input data are required. The pipeline manages pipeline
    components, applies node filtering and building logic, and executes transformations in parallel
    to produce output nodes.

    Attributes:
        inner_pipeline (IngestionPipeline): The internal ingestion pipeline used for processing
        the transformation components.
        num_workers (int): Number of parallel workers for processing the data. Defaults to the
        system's CPU count if not provided.
        batch_size (int): Size of batches for processing data. Defaults to a configured batch size.
        batch_writes_enabled (bool): Flag indicating whether batch writes are enabled.
        batch_write_size (int): Size of batches for processing writes. Defaults to a configured size.
        include_domain_labels (bool): Flag indicating whether domain labels should be included.
        node_builders (NodeBuilders): Object that encapsulates the logic for building nodes,
        applying filters, and formatting metadata.
        node_filter (NodeFilter): Filter used for excluding or including nodes based on certain conditions.
        pipeline_kwargs (dict): Additional keyword arguments passed to the pipeline.
    """
    @staticmethod
    def create(components: List[TransformComponent], 
               num_workers:Optional[int]=None, 
               batch_size:Optional[int]=None, 
               batch_writes_enabled:Optional[bool]=None, 
               batch_write_size:Optional[int]=None, 
               builders:Optional[List[NodeBuilder]]=[], 
               show_progress=False, 
               checkpoint:Optional[Checkpoint]=None,
               build_filters:Optional[BuildFilters]=None,
               source_metadata_formatter:Optional[SourceMetadataFormatter]=None,
               include_domain_labels:Optional[bool]=None,
               tenant_id:Optional[TenantId]=None,
               **kwargs:Any
            ):
        """
        Creates and initializes a `Pipe` object configured with the provided parameters and
        components through the `BuildPipeline`. This method facilitates seamless aggregation
        of processing components into a pipeline, enabling execution with specific settings
        such as concurrency, batching, filtering, and more.

        Args:
            components (List[TransformComponent]): List of transformation components to be
            included in the pipeline.
            num_workers (Optional[int]): Number of worker threads or processes for parallel
            execution of the pipeline. Defaults to None.
            batch_size (Optional[int]): Size of data batches to be processed. Defaults to None.
            batch_writes_enabled (Optional[bool]): Toggles batching for writes during the
            processing. Defaults to None.
            batch_write_size (Optional[int]): Size of data batches for write operations,
            applicable if batching is enabled. Defaults to None.
            builders (Optional[List[NodeBuilder]]): Optional list of node builders if specific
            building structures are required. Defaults to an empty list.
            show_progress (bool): Flag indicating whether to show progress during processing.
            Defaults to False.
            checkpoint (Optional[Checkpoint]): Optional checkpoint configuration to enable
            resumption from a specific state. Defaults to None.
            build_filters (Optional[BuildFilters]): Filters applied during the build process
            to refine data processing. Defaults to None.
            source_metadata_formatter (Optional[SourceMetadataFormatter]): Formatter for source
            metadata to customize metadata configuration. Defaults to None.
            include_domain_labels (Optional[bool]): Specifies whether domain labels should be
            incorporated in the output. Defaults to None.
            tenant_id (Optional[TenantId]): Identifier for tenant-specific operations or
            segregations. Defaults to None.
            **kwargs (Any): Additional keyword arguments to customize further configuration
                of the pipeline.

        Returns:
            Pipe: A configured `Pipe` instance encapsulating the constructed pipeline for
            execution.
        """
        return Pipe(
            BuildPipeline(
                components=components,
                num_workers=num_workers,
                batch_size=batch_size,
                batch_writes_enabled=batch_writes_enabled,
                batch_write_size=batch_write_size,
                builders=builders,
                show_progress=show_progress,
                checkpoint=checkpoint,
                build_filters=build_filters,
                source_metadata_formatter=source_metadata_formatter,
                include_domain_labels=include_domain_labels,
                tenant_id=tenant_id,
                **kwargs
            ).build
        )
    
    def __init__(self, 
                 components: List[TransformComponent], 
                 num_workers:Optional[int]=None, 
                 batch_size:Optional[int]=None, 
                 batch_writes_enabled:Optional[bool]=None, 
                 batch_write_size:Optional[int]=None, 
                 builders:Optional[List[NodeBuilder]]=[], 
                 show_progress=False, 
                 checkpoint:Optional[Checkpoint]=None,
                 build_filters:Optional[BuildFilters]=None,
                 source_metadata_formatter:Optional[SourceMetadataFormatter]=None,
                 include_domain_labels:Optional[bool]=None,
                 tenant_id:Optional[TenantId]=None,
                 **kwargs:Any
            ):
        """
        Initializes an instance of a class responsible for configuring and managing a data
        processing pipeline. This pipeline processes input through a customizable sequence
        of transformation components, while optionally supporting checkpointing, multiprocessing,
        and domain-specific metadata.

        Args:
            components (List[TransformComponent]): A list of transformation components that
            collectively define the pipeline workflow. Defaults to an empty list if None.
            num_workers (Optional[int]): The number of worker processes for parallel execution.
            Defaults to the system's processor count or a preconfigured value.
            batch_size (Optional[int]): The size of data batches to process in each step of the
            pipeline. Defaults to a preconfigured value.
            batch_writes_enabled (Optional[bool]): Indicates whether batch writes are enabled
            during processing. Defaults to a preconfigured value.
            batch_write_size (Optional[int]): Specifies the number of items to include in each
            batch write operation. Defaults to a preconfigured value.
            builders (Optional[List[NodeBuilder]]): A list of node builders used for node creation
            in the pipeline. Defaults to an empty list.
            show_progress (bool): Whether to display progress updates during processing.
            Defaults to False.
            checkpoint (Optional[Checkpoint]): An object for managing checkpoint operations,
            enabling the pipeline to resume from a specific state. Defaults to None.
            build_filters (Optional[BuildFilters]): Filters applied during node building,
            restricting or modifying node creation. Defaults to None.
            source_metadata_formatter (Optional[SourceMetadataFormatter]): A formatter for metadata
            associated with nodes, enabling customization or domain-specific adjustments.
            Defaults to a `DefaultSourceMetadataFormatter` instance.
            include_domain_labels (Optional[bool]): Indicates whether domain labels should be
            included in the output during processing. Defaults to a preconfigured value.
            tenant_id (Optional[TenantId]): An identifier for the tenant, used for scoping data.
            Defaults to None.
            **kwargs (Any): Additional keyword arguments to configure the pipeline behavior.
        """
        components = components or []
        num_workers = num_workers or GraphRAGConfig.build_num_workers
        batch_size = batch_size or GraphRAGConfig.build_batch_size
        batch_writes_enabled = batch_writes_enabled or GraphRAGConfig.batch_writes_enabled
        batch_write_size = batch_write_size or GraphRAGConfig.build_batch_write_size
        include_domain_labels = include_domain_labels or GraphRAGConfig.include_domain_labels
        source_metadata_formatter = source_metadata_formatter or DefaultSourceMetadataFormatter()
        
        for c in components:
            if isinstance(c, NodeHandler):
                c.show_progress = show_progress

        if num_workers > multiprocessing.cpu_count():
            num_workers = multiprocessing.cpu_count()
            logger.debug(f'Setting num_workers to CPU count [num_workers: {num_workers}]')

        if checkpoint and components:
            
            l = len(components)
            for i, c in enumerate(reversed(components)):
                updated_component = checkpoint.add_writer(c)
                if isinstance(updated_component, CheckpointWriter):
                    components[l-i-1] = updated_component
                    break

        logger.debug(f'Build pipeline components: {[type(c).__name__ for c in components]}')

        self.inner_pipeline=IngestionPipeline(transformations=components, disable_cache=True)
        self.num_workers = num_workers
        self.batch_size = batch_size
        self.batch_writes_enabled = batch_writes_enabled
        self.batch_write_size = batch_write_size
        self.include_domain_labels = include_domain_labels
        self.node_builders = NodeBuilders(
            builders=builders, 
            build_filters=build_filters, 
            source_metadata_formatter=source_metadata_formatter, 
            id_generator=IdGenerator(tenant_id=tenant_id)
        )
        self.node_filter = NodeFilter() if not checkpoint else checkpoint.add_filter(NodeFilter())
        self.pipeline_kwargs = kwargs
    
    def _to_node_batches(self, source_doc_batches:Iterable[Iterable[SourceDocument]]) -> List[List[BaseNode]]:
        """
        Converts batches of source documents into batches of nodes based on filtering and
        builder processes. Each batch of source documents is processed individually to form
        a batch of nodes.

        Args:
            source_doc_batches: A collection of collections, where each inner collection
            contains `SourceDocument` instances to be processed.

        Returns:
            A list of lists, where each inner list contains `BaseNode` objects that
            represent the processed and filtered nodes derived from the input source
            documents.
        """
        results = []
    
        for source_documents in source_doc_batches:
        
            chunk_node_batches = [
                self.node_filter(source_document.nodes)
                for source_document in source_documents
            ]

            node_batches = [
                self.node_builders(chunk_nodes) 
                for chunk_nodes in chunk_node_batches if chunk_nodes
            ]

            nodes = [
                node
                for nodes in node_batches
                for node in nodes
            ]   
        
            results.append(nodes)

        return results

    def build(self, inputs: Iterable[SourceType]):
        """
        Processes a set of input source types to generate and yield output nodes through a
        configurable build pipeline. The method organizes source documents into batches
        and processes them through a pipeline with customizable settings for parallel
        execution, batching, and additional pipeline-specific options.

        Args:
            inputs: An iterable of SourceType objects representing source data to be
            processed into nodes.

        Yields:
            BaseNode: Yields nodes as output from the processing pipeline, after being
            transformed and processed according to the pipeline steps.
        """
        input_source_documents = source_documents_from_source_types(inputs)

        for source_documents in iter_batch(input_source_documents, self.batch_size):

            num_source_docs_per_batch = math.ceil(len(source_documents)/self.num_workers)
            source_doc_batches = iter_batch(source_documents, num_source_docs_per_batch)
            
            node_batches:List[List[BaseNode]] = self._to_node_batches(source_doc_batches)

            logger.info(f'Running build pipeline [batch_size: {self.batch_size}, num_workers: {self.num_workers}, job_sizes: {[len(b) for b in node_batches]}, batch_writes_enabled: {self.batch_writes_enabled}, batch_write_size: {self.batch_write_size}]')

            output_nodes = run_pipeline(
                self.inner_pipeline,
                node_batches,
                num_workers=self.num_workers,
                batch_writes_enabled=self.batch_writes_enabled,
                batch_size=self.batch_size,
                batch_write_size=self.batch_write_size,
                include_domain_labels=self.include_domain_labels,
                **self.pipeline_kwargs
            )

            for node in output_nodes:
                yield node       

