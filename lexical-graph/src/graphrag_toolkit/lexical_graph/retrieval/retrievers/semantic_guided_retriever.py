# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import concurrent.futures
import logging
from collections import defaultdict
from typing import List, Optional, Any, Union, Type
from itertools import repeat

from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode

from graphrag_toolkit.lexical_graph.metadata import FilterConfig
from graphrag_toolkit.lexical_graph.storage.graph import GraphStore
from graphrag_toolkit.lexical_graph.storage.vector import VectorStore

from graphrag_toolkit.lexical_graph.retrieval.retrievers.semantic_guided_base_retriever import SemanticGuidedBaseRetriever
from graphrag_toolkit.lexical_graph.retrieval.retrievers.keyword_ranking_search import KeywordRankingSearch
from graphrag_toolkit.lexical_graph.retrieval.retrievers.statement_cosine_seach import StatementCosineSimilaritySearch
from graphrag_toolkit.lexical_graph.retrieval.retrievers.semantic_beam_search import SemanticBeamGraphSearch
from graphrag_toolkit.lexical_graph.retrieval.retrievers.rerank_beam_search import RerankingBeamGraphSearch
from graphrag_toolkit.lexical_graph.retrieval.utils.statement_utils import get_statements_query, SharedEmbeddingCache

logger = logging.getLogger(__name__)

SemanticGuidedRetrieverType = Union[SemanticGuidedBaseRetriever, Type[SemanticGuidedBaseRetriever]]

class SemanticGuidedRetriever(SemanticGuidedBaseRetriever):
    """
    Implementation of a retrieval class that combines semantic-guided search mechanisms to retrieve data
    from multiple sources, such as vector and graph stores. The class integrates various retrievers for
    initial search and graph-based expansion, offering a flexible and configurable retrieval system.

    This class is designed for retrieving nodes relevant to a given query, applying semantic similarity,
    keyword ranking, graph expansion, and filtering strategies. It ensures efficient and comprehensive
    information retrieval, leveraging shared caches, metadata filtering, and source grouping to organize
    the resulting nodes.

    Attributes:
        share_results (bool): Indicates if results from initial retrieval should be shared and used for
            graph-based expansion.
        shared_embedding_cache (SharedEmbeddingCache): Caches embeddings to optimize retrieval performance
            across multiple retriever instances.
        initial_retrievers (list): Contains instances of retrievers used for the initial search phase.
        graph_retrievers (list): Contains instances of graph-based retrievers for result expansion.
    """
    def __init__(
        self,
        vector_store:VectorStore,
        graph_store:GraphStore,
        retrievers:Optional[List[Union[SemanticGuidedBaseRetriever, Type[SemanticGuidedBaseRetriever]]]]=None,
        share_results:bool=True,
        filter_config:Optional[FilterConfig]=None,
        **kwargs: Any,
    ) -> None:
        """
        Initializes a composite retriever system that integrates multiple retriever classes for
        semantic and keyword-based search using shared embedding caches and vector/graph
        stores. The initialization process allows for custom retrievers or defaults to predefined
        retrievers with specific configurations.

        Args:
            vector_store: The vector storage backend used for embedding search.
            graph_store: The graph storage backend used for retrieving graph-based results.
            retrievers: Optional list of retriever instances or retriever classes. If classes
                are provided, they will be initialized with the provided `vector_store`,
                `graph_store`, and additional keyword arguments.
            share_results: Boolean flag indicating whether the results from different retrievers
                should be shared.
            filter_config: Optional configuration used for filtering retriever results.
            **kwargs: Additional keyword arguments used to initialize retrievers.

        """
        super().__init__(vector_store, graph_store, filter_config, **kwargs)

        self.share_results = share_results
        
        # Create shared embedding cache
        self.shared_embedding_cache = SharedEmbeddingCache(vector_store)

        self.initial_retrievers = []
        self.graph_retrievers = []
        
        # initialize retrievers
        if retrievers:
            for retriever in retrievers:
                if isinstance(retriever, type):
                    instance = retriever(
                        vector_store, 
                        graph_store, 
                        **kwargs
                    )
                else:
                    instance = retriever
                
                # Inject shared cache if not already set
                if hasattr(instance, 'embedding_cache') and instance.embedding_cache is None:
                    instance.embedding_cache = self.shared_embedding_cache
                
                if isinstance(instance, (SemanticBeamGraphSearch, RerankingBeamGraphSearch)):
                    self.graph_retrievers.append(instance)
                else:
                    self.initial_retrievers.append(instance)
        else:
            # Default configuration
            self.initial_retrievers = [
                StatementCosineSimilaritySearch(
                    vector_store=vector_store,
                    graph_store=graph_store,
                    embedding_cache=self.shared_embedding_cache,
                    **kwargs
                ),
                KeywordRankingSearch(
                    vector_store=vector_store,
                    graph_store=graph_store,
                    embedding_cache=self.shared_embedding_cache,
                    **kwargs
                )
            ]

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """
        Retrieves and processes nodes based on a query, leveraging multiple retrievers and
        optional graph expansion. The method executes in several stages, including initial
        retrieval, deduplication of nodes, optional graph-based expansion, fetching detailed
        statement data, filtering nodes based on metadata, and grouping nodes by their source
        for final ordering. It returns an ordered list of nodes, maximizing relevance and
        context.

        Args:
            query_bundle (QueryBundle): The query bundle containing the necessary information
                for performing the retrieval process.

        Returns:
            List[NodeWithScore]: A list of nodes ordered by score, with detailed metadata
                included for contextual relevance.
        """
        # 1. Get initial results in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.initial_retrievers)) as p:
            initial_results = list(p.map(
                lambda r, query: r.retrieve(query), 
                self.initial_retrievers, 
                repeat(query_bundle)
            ))
        
        if logger.isEnabledFor(logging.DEBUG) and self.debug_results:
            logger.debug(f'initial_results: {initial_results}')
        else:
            logger.debug(f'num initial_results: {len(initial_results)}')

        # 2. Collect unique initial nodes
        seen_statement_ids = set()
        initial_nodes = []
        for nodes in initial_results:
            for node in nodes:
                statement_id = node.node.metadata['statement']['statementId']
                if statement_id not in seen_statement_ids:
                    seen_statement_ids.add(statement_id)
                    initial_nodes.append(node)

        all_nodes = initial_nodes.copy()

        if logger.isEnabledFor(logging.DEBUG) and self.debug_results:
            logger.debug(f'all_nodes (before expansion): {all_nodes}')
        else:
            logger.debug(f'num all_nodes (before expansion): {len(all_nodes)}')

        # 3. Graph expansion if enabled
        if self.share_results and initial_nodes:
            for retriever in self.graph_retrievers:
                try:
                    retriever.shared_nodes = initial_nodes
                    graph_nodes = retriever.retrieve(query_bundle)
                    for node in graph_nodes:
                        statement_id = node.node.metadata['statement']['statementId']
                        if statement_id not in seen_statement_ids:
                            seen_statement_ids.add(statement_id)
                            all_nodes.append(node)
                except Exception as e:
                    logger.error(f"Error in graph retriever {retriever.__class__.__name__}: {e}")
                    continue

        if logger.isEnabledFor(logging.DEBUG) and self.debug_results:            
            logger.debug(f'all_nodes (after expansion): {all_nodes}')
        else:
            logger.debug(f'num all_nodes (after expansion): {len(all_nodes)}')

        # 4. Fetch statements once
        if not all_nodes:
            return []

        statement_ids = [
            node.node.metadata['statement']['statementId'] 
            for node in all_nodes
        ]
        statements = get_statements_query(self.graph_store, statement_ids)

        if logger.isEnabledFor(logging.DEBUG) and self.debug_results:
            logger.debug(f'statements: {statements}')
        else:
            logger.debug(f'num statements: {len(statements)}')
        

        # 5. Create final nodes with full data
        final_nodes = []
        statements_map = {
            s['result']['statement']['statementId']: s['result'] 
            for s in statements
        }
        
        for node in all_nodes:
            statement_id = node.node.metadata['statement']['statementId']
            if statement_id in statements_map:
                result = statements_map[statement_id]
                new_node = TextNode(
                    text=result['statement']['value'],
                    metadata={
                        **node.node.metadata,  # Preserve retriever metadata
                        'statement': result['statement'],
                        'chunk': result['chunk'],
                        'source': result['source']                     
                    }
                )
                final_nodes.append(NodeWithScore(
                    node=new_node,
                    score=node.score
                ))

        if logger.isEnabledFor(logging.DEBUG) and self.debug_results:       
            logger.debug(f'final_nodes: {final_nodes}')
        else:
            logger.debug(f'num final_nodes: {len(final_nodes)}')

        # 6. Apply metadata filters
        filtered_nodes = [
            node 
            for node in final_nodes 
            if self.filter_config.filter_source_metadata_dictionary(node.node.metadata['source']['metadata'])    
        ]

        if logger.isEnabledFor(logging.DEBUG) and self.debug_results:       
            logger.debug(f'filter_nodes: {filtered_nodes}')
        else:
            logger.debug(f'num filter_nodes: {len(filtered_nodes)}')

        # 7. Group by source for better context
        source_nodes = defaultdict(list)
        for node in filtered_nodes:
            source_id = node.node.metadata['source']['sourceId']
            source_nodes[source_id].append(node)

        # 8. Create final ordered list
        ordered_nodes = []
        for source_id, nodes in source_nodes.items():
            nodes.sort(key=lambda x: x.score or 0.0, reverse=True)
            ordered_nodes.extend(nodes)

        if logger.isEnabledFor(logging.DEBUG) and self.debug_results:    
            logger.debug(f'ordered_nodes: {ordered_nodes}')
        else:
            logger.debug(f'num ordered_nodes: {len(ordered_nodes)}')

        return ordered_nodes
